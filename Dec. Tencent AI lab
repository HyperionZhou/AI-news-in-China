# Just write the script, AI understands your heart, from material selection to editing, one-stop completion of creation

The camera switches naturally and the tempo is slow. What kind of professional editing skills are required to create such a high-quality video?
The answer is that you can write without cutting.
For example, to edit a clip about a giraffe, you just need to tell the AI:
The giraffe is the world ’s tallest animal, and well known for its long legs and neck.
The giraffe is the tallest animal in the world and is famous for its long legs and long neck.
It has a brown mane on the neck, and its head has two hairy horns.
It has a brown mane on its neck and two furry horns on its head.
AI can automatically edit such a picture
Let's first show the full picture of the giraffe herd. Then the camera cuts a running giraffe to show its long legs and neck. Follow the instructions and give a close-up of your mane and horns, live together!
Yes, no longer hassle to pick the material, and go back and forth frame by frame

You only need to turn the ideas in your mind into short texts. This AI called Write-A-Video can understand your heart, from material selection to editing.
This is the latest AI artifact created by scientists from Beihang University, Tsinghua University, Harvard, and Herzliya Interdisciplinary Research Center in Israel. It can generate videos with only themed scripts.

Create video with text
Now that the material is ready, let's take a look at how Write-A-Video "writes" a video.
The whole process is divided into three steps.
In the first step, the user provides input in the form of text. Write-A-Video picks out keywords in sentences.
In the second step, Write-A-Video will use keywords to select candidate clips that match it in the material library.
Visual semantic matching between text and shots mainly includes two steps: keyword matching and visual semantic embedding.
First, the AI ​​will retrieve all video footage with keyword tags from the material library according to the script.
All videos in the material library are divided into snapshots for semantic matching, and users can use movie terminology to guide AI creation when the video is officially edited in the next step.

The author of Write-A-Video, teacher Wang Miao of Beijing University of Aeronautics and Astronautics, said that Write-A-Video allows users to use film terms in the input text to explore different visual styles of each scene, such as adjusting the rhythm of the movie Screen movement, etc.
In addition, using a histogram-based segmentation algorithm, if the histogram in the HSV color gamut between frames differs by more than 80%, and the tracked SURF key points do not match more than 80%, the algorithm will use these two frames as Circles, split the lens.
Lenses that are too long (> 30s) or too short (<2s) will also be discarded because short lenses will not look good, while long lenses will reduce efficiency and variability
Then, the visual semantic embedding technology calculates a matching score for each retrieved shot, and the shot with the highest ranking is selected as a candidate shot.
The method used here is VSE ++ (the address of the paper is at the end of the article). This method can encode cross-model content into a joint feature space, such as linking text to shots.
In the embedding space, one frame is taken every ten frames from the shot, the cosine similarity between it and the text is calculated, and the average value is finally taken as the final score for judging whether the shot can be selected.

The third step is to combine these shots together to complete the video editing.
This step is actually a hybrid optimization of the lens by Write-A-Video. And it has its own aesthetic standards.

First, the picture should be bright and vivid.
Secondly, the lens should not be shaken too much.
Finally, avoid inconsistent jump cuts and opposite camera movements.
It is worth mentioning that Write-A-Video is very user-friendly
Add new text, and the corresponding material will be automatically added.
Finding corresponding shots, cutting, rearranging, can be done in the form of text editing such as adding and deleting text and moving sentences. No need to master editing skills, no need to type code.
Not only that, in the video below, you will find that the typed text can be turned into a voice-over narration to render the movie atmosphere.
And the narration and video are fully synchronized, so the correspondence is very natural. When it comes to Buckingham Palace, the camera naturally turns to the front of Buckingham Palace. When referring to the Royal Guard, the picture shows the Royal Guard during the performance.
The research team says that compared to commercial frame-by-frame video editors, using Write-A-Video makes creation much faster.
With Write-A-Video, even novices can complete video editing tasks at a much faster speed (13 minutes: 7 hours) with a quality that is not much different from professional editors.
Pr for editors, Write-A-Video for novices
At the just-concluded SIGGRAPH Asia 2019 conference, the research team reported and demonstrated the results of Write-A-Video, which was widely recognized by international peers.
Presented by the Tsinghua team of Beihang University
Dr. Wang Miao, the first author of Write-A-Video, is currently an assistant researcher and master's tutor of the State Key Laboratory of Virtual Reality Technology and Systems at Beijing University of Aeronautics and Astronautics.
△ Wang Miao
He graduated from Xi'an University of Electronic Science and Technology with a bachelor's degree, and later received a Ph.D. from Tsinghua University in 2016. He studied under the correspondence author of the thesis and Professor Hu Shimin from the Department of Computer Science of Tsinghua University.
The author of the thesis, Yang Guowei, was only an undergraduate when he participated in the project, and he is now pursuing a doctorate degree in computer science at Tsinghua University.
The other two authors are the Fields Prize winner, Harvard professor, Chinese American mathematician Qiu Chengtong, and Dean of Israel's Herzliya Interdisciplinary Research Center, Ariel Shamir.


# Sudden deaths among young people occur frequently, can AI doctors better avoid tragedies?
Zweig once said: "When a person is young, he always thought that illness and death would only patronize others."
However, according to incomplete statistics, about 550,000 people die suddenly each year in China. And more than 90% of young people are sub-healthy, and more than half of white-collar workers are at risk of overwork. For young people, in addition to avoiding unhealthy lifestyles such as long-term fatigue and staying up late, it is particularly important to actively prevent and reduce the factors that induce sudden death. Screening and diagnosis of cardiovascular diseases is one of them.
However, in the current medical market, the length of medical image screening and the lack of physician resources make it difficult to match the mass market for cardiovascular disease, so AI has its place.
Cardiovascular field urgently needs AI image to join
Not long ago, a study in The Lancet stated that during the 27 years from 1990 to 2017, the only Chinese resident who had remained the first place to die was mainly stroke (stroke), followed by ischemic heart Disease, chronic obstructive pulmonary disease, lung cancer, Alzheimer's disease, and other dementias.
According to China's cardiovascular disease data in 2018, the prevalence of cardiovascular disease is still on the rise. Currently, the number of patients suffering from cardiovascular disease is about 290 million. Cardiovascular disease deaths account for more than 40% of residents' disease deaths. Disease first. And from a global perspective, according to the World Health Organization (WHO) statistics, ischemic heart disease and stroke are also the world ’s leading causes of death, and the second leading cause of death, which shows that cardiovascular disease has now become a global problem. Major public health issues.
In order to curb this trend, the best way is to promote early cardiovascular screening, and to identify potential cardiovascular problems early. Medical imaging is the most important clinical diagnosis and differential diagnosis tool in modern medicine. Nearly 70% of clinical diagnosis requires its use, especially in the cardiovascular field, which has a wide range of needs.
However, compared with the exuberance of cardiovascular "market demand", the traditional medical system is struggling to cope. Generally speaking, cardiovascular imaging diagnosis is divided into four steps: image scanning, image post-processing, report writing, and report review. The time spent in the imaging process of a single case is basically half an hour. "Cardiovascular post-processing is very troublesome. Even the medical equipment workstations of traditional international leading manufacturers such as Philips and Siemens cannot be completed quickly and are labor-intensive. Take Anzhen Hospital (referring to Beijing Anzhen Hospital of Capital Medical University, its heart The Medical Center is one of the national diagnosis and treatment centers for difficult and critical cardiovascular diseases.) For example, it may take nearly 200 cardiovascular imaging diagnoses a day, and it usually takes half an hour for doctors and at least 15 minutes. "Focus on cardiovascular Liu Jian, vice president of strategy for Shukun Technology, an AI medical technology company in the field, told the AI ​​report.
In developed countries in Europe and the United States, the average number of doctors receiving a doctor is not large, so even if AI makes screening and diagnosis faster, it will not be very attractive. However, in China, the annual increase of medical images exceeds 30%, and only 4% of new radiologists are added each year, and the gap between imaging doctors is huge. Moreover, China's medical resources are currently showing an "upside down" trend. According to the National Health and Medical Commission, as of the end of November 2018, less than 8% of tertiary hospitals had visited 1.646 billion people, accounting for nearly 60% of the country's total. Some Beijing residents have reported that they usually have to wait for a week or two to go to the top three hospitals to see their senior directors. This is even more difficult for cardiovascular diseases where time is critical.
In contrast, AI imaging can complete diagnosis in three minutes or even faster. Facing the huge market demand, AI has high hopes for cardiovascular imaging diagnosis.
Avoid AI images "Red Sea
Facing the broad market prospects of cardiovascular imaging AI, it is imagined that its related AI companies and products should be as big as cattle hair. However, this is not the case. Currently the industry's most concentrated product is still pulmonary nodule screening. "Investigation on the status and needs of the medical imaging AI industry" shows that 88% of the AI ​​products used in the department are lung nodule screening, 6% are coronary artery analysis, and the rest are bone age, breast and prostate intelligent diagnosis.
In terms of the number of AI medical companies, media surveys show that more than 140 companies engaged in medical AI, nearly 120 are doing medical imaging business, of which about 100 companies cut into pulmonary nodule imaging products. There are many players on the festival track.
"Pulmonary nodules are also a common clinical problem. There are more open data and open source algorithms. For enterprises, it is relatively easy to enter. Now the market is a red sea." Liu Jian said, "So we are special Alone, chose to cut through the cardio-cerebral vascular field. "
In the industry, lung nodule screening is the entry level for technology companies. In contrast, the diagnostic process of cardiovascular disease is more complicated, and the technical threshold is too high. For example: coronary CT images need to undergo complex three-dimensional reconstruction. From the aorta to the coronary blood vessels, the diameter of the blood vessels varies from centimeters to millimeters by an order of magnitude. To diagnose the origin, shape of the blood vessels, plaques on the vessel wall, and lumen Narrow and so on, this is a difficult and challenging scene. Faced with complex tasks, the currently adopted deep learning models are incapable. In addition, whether there is a high-standard, high-quality annotation database that supports this field is also a key element to be considered.
"The AI ​​images used for lung nodule screening involve 200 million neurons, and our multi-dimensional convolutional neural network technology for cardiovascular screening has 1 billion neurons," Liu Jian said. It can accurately segment human organs, reconstruct clear and accurate 3D digital hearts, digital brains, etc., and make accurate diagnosis of relevant parts.
For example, "CoronaryDoc, the first AI-assisted diagnostic system for coronary CT imaging in China", for example, the system can automatically capture CT images during cardiovascular treatment, and perform 3D reconstruction, interpretation, and output of standardized reports. This process only takes 3 minutes. Can be done. Later, the doctor only needs to confirm the result given by the AI, or modify it according to his own judgment. There is no need to confirm it in steps, which greatly improves the efficiency.
"Taking DSA doctor's results as the gold standard, in detecting lesions, coronary AI sensitivity is higher than 95% (simple understanding: high sensitivity = low rate of missed diagnosis) is higher than doctors' interpretation. Product specificity (simple understanding: High specificity = low misdiagnosis rate) is similar to the results of CTA arbitrators. This means that most screenings are not missed with AI screening. "Liu Jian said that AI coronary CTA greatly reduces the rate of missed diagnosis of stenotic lesions. The accuracy and specificity of AI diagnosis is no less than that of senior expert doctors in Tier 3 hospitals in first-tier cities. The interpretation efficiency of AI and doctors is significantly better than that of human interpretation alone.

Although AI healthcare is developing rapidly, it is still slower than other industries. Chen Juan, the chairman of Youjiali, famous for remote ECG data monitoring, shared a story in a forum: once she went to promote a product to an Anhui hospital, but the director of cardiology first asked her to see her first. The question is "Can your company live for five years?" Because the director had purchased a batch of ECG equipment, but only two years later, the equipment company closed down.
In the promotion of AI medical companies, the products were rejected by doctors as "anti-human" when they were promoted by doctors, and some doctors declined because they were worried about being replaced by AI.
Fortunately, doctors will slowly change. Liu Xingpeng, deputy director of the Heart Center of Beijing Chaoyang Hospital and director of the arrhythmia department, is a doctor representative who supports AI-enabled medical care. Liu Xingpeng said, "The doctor community is slow, but if AI can really stimulate him, he is also very willing to accept it."
From knowing AI, accepting AI, and embracing AI products, doctors like Liu Xingpeng have also started to increase. Doctors will no longer worry about whether they have been replaced, nor are they afraid that AI companies that have tried products will die early. Instead, they will start to combine their own experiences and put forward more specific requirements for artificial intelligence.
The value of AI for medical auxiliary diagnosis is increasingly prominent. Liu Jian pointed out that our country has always emphasized the construction of heart centers, which is very important for the improvement of primary medical care. Artificial intelligence helps grassroots doctors solve problems they cannot and cannot diagnose. For large hospitals, it can solve the problem of circulation.
New development path
Although the products are getting more and more recognized, "how many hospitals are there" and "what is the business model" is still a difficult problem for every medical AI company.
Zhou Ye, Managing Director of Junlian Capital, said that artificial intelligence technology is very advanced, but artificial intelligence is very backward in business models in medical applications. "It is unrealistic to charge patients once with software. How to set up a charging model needs to be broken."
From the perspective of medical veteran Liu Jian, the AI ​​medical industry is different from other industries. The current model is to enter the hospital by "free trial" first, and then discuss commercialization issues after being recognized. At present, Shukun Technology is undergoing commercial transformation.



#Can artificial intelligence really learn "mind reading"? If so, how?
AI understands thinking is the trend
Automated and non-invasive detection of brain activity may be of great use in areas such as human-computer interaction and mental health. It can provide an extra dimension of interaction between the user and the device, and it can also export physical information that does not rely on oral communication.
Such innovation also means better brain-computer interfaces. This will open up entirely new platforms for human-machine communication, including providing support for people with physical or mental illness. A brain-computer interface allows a paralyzed person to move a robotic arm or a spinal cord injured person to control an electric wheelchair.

As people increasingly get low-cost electroencephalography (EEG) equipment, the consumer industry and research institutions can afford the cost of EEG data, which has created a need to replace human experts with automatic classification.

This article will introduce a case study on how to use machine learning to analyze brain activity. Using EEG recordings of commercially available equipment, the article will show how to use machine learning models to predict the corresponding mental state of a subject.
How machine learning classifies mental states
Unless the recorder belongs to a laboratory performing such experiments, recording high-quality EEG data is not easy. However, I recently saw an interesting article, co-authored by Jordan Bird, Luis Manso, Eduardo Ribeiro, Anico Ecate and Diego R. Faria Research on Mental State Classification Using EEG-Based Brain-Computer Interface. Fortunately, they have publicly shared the data used in the study for others to experiment with. What's particularly interesting is that you can place orders for consumer-grade devices for only a few hundred dollars on Amazon. The methods used to record and process data in their research are described in the following sections.
 
Experimental details
 
The study utilized a commercially available MUSE brand EEG headband using four dry extracranial electrodes. This is a wearable brain sensing device that measures brain activity with 4 electroencephalogram (EEG) sensors.

In order to evoke different mental states, the experiment used a series of movie clips shown in the table below, which represent positive and negative titers, respectively.
 
For each video clip in the table, 60 seconds of data were recorded from two subjects (1 male, 1 female, aged 20-22) to generate 12 minutes of brain activity data (corresponding to each emotional state 6 minutes of data). In addition, six minutes of neutral EEG data were collected to generate a total of up to 36 minutes of EEG data recorded from the subject. (Collect data for three minutes each day to reduce interference with resting emotional states). By resampling the variable frequency to 150Hz, a data set with a capacity of 324,000 data points can be generated.
 
Feature extraction and EEG signal classification are the core issues of brain-computer interface (BCI) applications. One of the challenges in EEG feature extraction is the complexity of the signal because it is actually non-linear, unstable, and arbitrary. Only between extremely short intervals is the signal considered stable. That's why applying short-term windowing techniques to meet your needs is a best practice. However, it is still considered a hypothesis that is established under normal brain conditions.

This section shows feature sets that are thought to properly distinguish between different categories of mental state. These features depend on statistical techniques, time-frequency analysis based on fast Fourier transform, information entropy, and maximum-minimum characteristics of time series. According to the temporal distribution of signals in a given time window, all features initially used to classify mental states will be calculated. The time period of the sliding window is defined as 1 second, that is, all features are calculated at this time point.

Machine learning algorithms
 
Following the above feature extraction method from the original EEG data, we now have a data set containing 2547 features. For each row of the data set, there is a corresponding target variable: "Neutral", "Negative" or "Positive". The experimental goal is to train a machine learning model based on this set of features to successfully predict the corresponding mental state.
This example starts with the "go-to" algorithm of random forest classification, because it is simple to set up, has powerful out-of-the-box features, and does not require hyperparameter tuning.
Here's the explanation: it would also be interesting to use convolutional neural network methods to process raw time-domain data (not extracted feature sets that contain various frequency characteristics of the signal). Since the convolution applied in the time domain is closely related to the frequency characteristics of the signal in the convolution theorem, this is likely to be an effective method to reduce the workload of preprocessing and feature extraction.

Cross-validation
 
In the process of applying machine learning, you first need to understand the importance of cross-validation: evaluating the performance of a model on a part of the data set, which needs to be distinguished from the part used to train the model. One way is to keep part of the data set while training the model, and evaluate the model performance in the following way (example):

Train the model with 70% labeled data

Evaluate the trained model with the remaining 30%

Through repeated iterations, it is observed whether the test results are different due to different training / test samples, and K-fold cross-validation is thus improved.


 
Through multiple training / test comparisons, better model performance evaluation and complete inspection can be obtained, and it is confirmed that after training with different labeled data segments, the model performance will not be very different, even if the data itself is in There is instability in the model or the sample set is small.

In this case, a 10-fold cross-validation was used when training the model, and the accuracy of the evaluation on different data segments was calculated. The final model performance can be visualized through the following confusion matrix.

In the field of machine learning and specific statistical classification problems, the confusion matrix, also known as the error matrix, is a special form layout that can visualize the algorithm. Each row of the matrix represents an instance of the actual class, and each column represents an instance of the predicted class. Its name comes from its ability to easily see if the system is confusing different classes (for example, mislabeling one class as another).

When cross-validation is more than 10% off for prediction of the evaluation model, the final result obtained is impressive-0.987 (+/- 0.01) accuracy. This shows that based on the feature set extracted from the original EEG data, the prediction of a person's psychological state is "neutral", "negative" or "positive", with an accuracy rate of nearly 99%.

Predicting emotional state through EEG recordings. Although the results of this example are moving, there is still work to be done for wider applications. At the same time, the limited sample size of only two subjects in the experimental records also raises the problem of generalizing new individuals. However, due to the promising prospects of the sample results, it is also a good start for further research.


Outlook
 
Does this result imply that people are approaching a future of ectopic science fiction with completely controlled thoughts? In this future world, people will even lose the right to privacy.

The ability of artificial intelligence to read human brain activity raises ethical questions about privacy and security, and key researchers need to recognize this. Technology opens the Pandora's box of new malicious applications, including stealing them from the human brain after manipulating it to think about sensitive information. However, breakthroughs in this area still take time, and an important step needs to be taken to crack the thinking-based brain model. Such innovation also means better brain-computer interfaces. This will open up a new platform for human-machine communication. A brain-computer interface allows a paralyzed person to move a robotic arm or a spinal cord injured person to control an electric wheelchair. Applications such as "smart repair" shown in the video below are a big step forward in helping people with disabilities around the world.


Beyond Bionics, The Guardian
 
Although he considers himself a "tech optimist", he still believes that it is necessary to improve the laws and regulations of the technology to ensure that the technology really helps those in need without causing disastrous consequences.

What the future holds, we cannot predict. But one thing is certain, people are deliberately or unintentionally looking for ways to interact with computers. Artificial intelligence-driven interfaces are promising and challenging.

Now that Pandora's Box has been opened, go ahead bravely!

"AI companies have become a common problem in this industry. Enterprises that have nothing to do with artificial intelligence are now packaged as artificial intelligence companies for publicity." Zhou Zheng said, "Many companies doing artificial intelligence now only use open source algorithms, and There is no need to customize it to optimize the algorithm. "

Zhou Zheng also pointed out the pain points of many medical AI companies, including the video track. "Many companies are uneven in terms of data, and the quality of the data needs to be verified. Some companies even use both data and training The algorithm is used for verification again, and the results must be considerable but not objective. A slightly better company separates the training data from the verification data. However, more stringent methods require third-party verification, such as data from 301 hospital At the time of the final verification, the data of Concord Hospital were used for verification. "

In this regard, Liu Jian also deeply acknowledged that, as a representative of medical AI companies, he emphasized, "Enterprises must be clean and good, control data sources, and integrate with doctors to allow doctors to enter data and control quality."

While seeking data validity, medical AI companies have also begun to seek new development paths. The layered diagnosis and treatment system allows AI medical startups to shift their sights from the already saturated Beijing-Guangzhou-Guangzhou top three hospitals and sink to the primary medical market. Combining with GPS (Philips, Siemens, GE Medical) and other large equipment manufacturers is also one of the breakthrough ways.
